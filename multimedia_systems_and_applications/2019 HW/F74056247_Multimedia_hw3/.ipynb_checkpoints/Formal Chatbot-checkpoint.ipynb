{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.744 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成前 10000 行的斷詞\n",
      "已完成前 20000 行的斷詞\n",
      "已完成前 30000 行的斷詞\n",
      "已完成前 40000 行的斷詞\n",
      "已完成前 50000 行的斷詞\n",
      "已完成前 60000 行的斷詞\n",
      "已完成前 70000 行的斷詞\n",
      "已完成前 80000 行的斷詞\n",
      "已完成前 90000 行的斷詞\n",
      "已完成前 100000 行的斷詞\n",
      "已完成前 110000 行的斷詞\n",
      "已完成前 120000 行的斷詞\n",
      "已完成前 130000 行的斷詞\n",
      "已完成前 140000 行的斷詞\n",
      "已完成前 150000 行的斷詞\n",
      "已完成前 160000 行的斷詞\n",
      "已完成前 170000 行的斷詞\n",
      "已完成前 180000 行的斷詞\n",
      "已完成前 190000 行的斷詞\n",
      "已完成前 200000 行的斷詞\n",
      "已完成前 210000 行的斷詞\n",
      "已完成前 220000 行的斷詞\n",
      "已完成前 230000 行的斷詞\n",
      "已完成前 240000 行的斷詞\n",
      "已完成前 250000 行的斷詞\n",
      "已完成前 260000 行的斷詞\n",
      "已完成前 270000 行的斷詞\n",
      "已完成前 280000 行的斷詞\n",
      "已完成前 290000 行的斷詞\n",
      "已完成前 300000 行的斷詞\n",
      "已完成前 310000 行的斷詞\n",
      "已完成前 320000 行的斷詞\n",
      "已完成前 330000 行的斷詞\n",
      "已完成前 340000 行的斷詞\n",
      "已完成前 350000 行的斷詞\n",
      "已完成前 360000 行的斷詞\n",
      "已完成前 370000 行的斷詞\n",
      "已完成前 380000 行的斷詞\n",
      "已完成前 390000 行的斷詞\n",
      "已完成前 400000 行的斷詞\n",
      "已完成前 410000 行的斷詞\n"
     ]
    }
   ],
   "source": [
    "# 斷詞部分\n",
    "output = open('./stopword.txt', 'w', encoding='utf-8')\n",
    "with open('./Dataset.txt', 'r', encoding='utf-8') as content :\n",
    "    for texts_num, line in enumerate(content):\n",
    "        line = line.strip('\\n')                     # 去除換行符號\n",
    "        words = jieba.cut(line, cut_all=False)      # 用 jieba 斷詞\n",
    "        for word in words:                          # 如果斷詞的字是 stopwords 將它去除\n",
    "            output.write(word + ' ')\n",
    "        output.write('\\n')\n",
    "        if (texts_num + 1) % 10000 == 0:            # 每 10000 行顯示進度\n",
    "            print(\"已完成前 %d 行的斷詞\" % (texts_num + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = word2vec.LineSentence('./stopword.txt')\n",
    "model = word2vec.Word2Vec(sentences,size = 400, window = 20, workers = 3, sg = 1, min_count=0, iter=300)\n",
    "model.save('stupid.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"stupid.bin\" # 載入模組\n",
    "model_w2v = word2vec.Word2Vec.load(model)\n",
    "\n",
    "# 設定 output\n",
    "outputfile = open('F74056247.csv', 'w+', encoding='utf-8')\n",
    "# 讀入題目\n",
    "with open(\"project_question_file.txt\", encoding='utf-8')as inputline:\n",
    "    for line in inputline:\n",
    "        line = line.strip('\\n')\n",
    "        output = line.split(\"\\t\", 1)\n",
    "        text = output[0] \n",
    "        answer = output[1].split(\"\\t\")\n",
    "        \n",
    "\n",
    "        words = list(jieba.cut(text.strip()))\n",
    "        word = []   # 當前的題目儲存在這裡\n",
    "        for w in words:     # 去除 stopword\n",
    "            if w in model_w2v.wv.vocab:\n",
    "                word.append(w)\n",
    "        \n",
    "        eachans = []    #每題的四個選項儲存在這裡\n",
    "        # 以 jieba 切割每個選項，並去除 stopword 之後再儲存成 list\n",
    "        for everyans in answer: \n",
    "            answercut = []\n",
    "            temp1 = \"\".join(everyans.split(')')[1]) \n",
    "            answercuts = jieba.cut(temp1, cut_all=False) \n",
    "            for checkvocab in answercuts:\n",
    "                if checkvocab in model_w2v.wv.vocab:\n",
    "                    answercut.append(checkvocab)\n",
    "            eachans.append(list(answercut))\n",
    "\n",
    "        score = []\n",
    "        if not eachans[0]: \n",
    "        score.append(model_w2v.wv.n_similarity(word, eachans[0]))\n",
    "        score.append(model_w2v.wv.n_similarity(word, eachans[1]))\n",
    "        score.append(model_w2v.wv.n_similarity(word, eachans[2]))\n",
    "        score.append(model_w2v.wv.n_similarity(word, eachans[3]))\n",
    "        choose = np.argmax(score) + 1\n",
    "        \n",
    "        outputfile.write('[' + str(choose) + ']\\n')\n",
    "        \n",
    "outputfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
